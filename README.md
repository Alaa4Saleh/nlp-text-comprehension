# Minimal Text Comprehension: Neural Evidence for POS Hierarchies

**Which parts of speech are essential for neural language understanding?**  
This repository provides both structured and unstructured analyses of POS hierarchies, combining cross-embedding validation with fMRI brain encoding tasks.

Content words maintain 85â€“90% of neural prediction accuracy, while function words drop to only 17%.

---

## ğŸ“‚ Files
- `structured_tasks.ipynb` â€“ Tasks 1â€“3: cross-embedding validation and brain encoding
- `unstructured_analysis.ipynb` â€“ core POS filtering analysis and semantic categorization
- `figures/` â€“ result figures generated by the notebooks (optional)

---

## ğŸ”‘ Key Findings
- Clear neural hierarchy: **content words > nouns + verbs > individual categories > function words**
- Content words alone sufficient for robust brain prediction (72.6% top-1 accuracy)
- 52-point mean rank difference between best (content words: 8.25) and worst (function words: 70.70) strategies

---

## ğŸ“Š Data Source
This analysis uses fMRI datasets from Pereira et al. (2018), containing 627 sentences with corresponding brain activity.  

> âš ï¸ **Note:** The dataset is **not included** in this repository.  
> Users must obtain the data directly from the authorsâ€™ dataset release.

---

## âš™ï¸ Setup
Clone the repository and install dependencies:

```bash
git clone https://github.com/USERNAME/minimal-text-comprehension.git
cd minimal-text-comprehension
pip install -r requirements.txt
