# Minimal Text Comprehension: Neural Evidence for POS Hierarchies

**Which parts of speech are essential for neural language understanding?**  
This repository provides both structured and unstructured analyses of POS hierarchies, combining cross-embedding validation with fMRI brain encoding tasks.

Content words maintain 85–90% of neural prediction accuracy, while function words drop to only 17%.

---

## 📂 Files
- `structured_tasks.ipynb` – Tasks 1–3: cross-embedding validation and brain encoding
- `unstructured_analysis.ipynb` – core POS filtering analysis and semantic categorization
- `figures/` – result figures generated by the notebooks (optional)

---

## 🔑 Key Findings
- Clear neural hierarchy: **content words > nouns + verbs > individual categories > function words**
- Content words alone sufficient for robust brain prediction (72.6% top-1 accuracy)
- 52-point mean rank difference between best (content words: 8.25) and worst (function words: 70.70) strategies

---

## 📊 Data Source
This analysis uses fMRI datasets from Pereira et al. (2018), containing 627 sentences with corresponding brain activity.  

> ⚠️ **Note:** The dataset is **not included** in this repository.  
> Users must obtain the data directly from the authors’ dataset release.

---

## ⚙️ Setup
Clone the repository and install dependencies:

```bash
git clone https://github.com/USERNAME/minimal-text-comprehension.git
cd minimal-text-comprehension
pip install -r requirements.txt
